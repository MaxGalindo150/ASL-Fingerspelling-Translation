{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Load the necessary packages","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport json","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_DIR = '../input/asl-fingerspelling'\n\ntrain = pd.read_csv(f'{BASE_DIR}/train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-06-06T03:43:02.665845Z","iopub.execute_input":"2023-06-06T03:43:02.666218Z","iopub.status.idle":"2023-06-06T03:43:02.827768Z","shell.execute_reply.started":"2023-06-06T03:43:02.666188Z","shell.execute_reply":"2023-06-06T03:43:02.826644Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Remove pose and face columns ","metadata":{}},{"cell_type":"markdown","source":"**Due to the computational power restrictions, we will only analyze 9 of the provided Parquet files, which amounts to approximately 9,000 sentences.**","metadata":{}},{"cell_type":"code","source":"parquet_files = ['train_landmarks/1134756332.parquet',\n 'train_landmarks/5414471.parquet',\n 'train_landmarks/1664666588.parquet',\n 'train_landmarks/1133664520.parquet',\n 'train_landmarks/234418913.parquet',\n 'train_landmarks/566963657.parquet',\n 'train_landmarks/1920330615.parquet',\n 'train_landmarks/105143404.parquet',\n 'train_landmarks/933868835.parquet']","metadata":{"execution":{"iopub.status.busy":"2023-06-06T03:43:03.889689Z","iopub.execute_input":"2023-06-06T03:43:03.890094Z","iopub.status.idle":"2023-06-06T03:43:03.895143Z","shell.execute_reply.started":"2023-06-06T03:43:03.890059Z","shell.execute_reply":"2023-06-06T03:43:03.894084Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**We keep each dataframe corresponding to each parquet file in a list called 'dataframes'**","metadata":{}},{"cell_type":"code","source":"dataframes = []\n\n# Iterar sobre cada archivo parquet\nfor file in parquet_files:\n    # Leer el archivo parquet en un DataFrame\n    df = pd.read_parquet(f'{BASE_DIR}/{file}')\n\n    # Agregar el DataFrame a la lista\n    dataframes.append(df)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T03:43:05.593746Z","iopub.execute_input":"2023-06-06T03:43:05.594375Z","iopub.status.idle":"2023-06-06T03:45:29.069965Z","shell.execute_reply.started":"2023-06-06T03:43:05.594342Z","shell.execute_reply":"2023-06-06T03:45:29.068448Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**We removed some columns.**","metadata":{}},{"cell_type":"code","source":"columns = ['x_pose_', 'y_pose_', 'z_pose_', 'x_face_', 'y_face_', 'z_face_']\n\nfor i in range(len(dataframes)):\n    dataframes[i] = dataframes[i].drop(columns=[col for col in dataframes[i].columns if any(col.startswith(column) for column in columns)])\n","metadata":{"execution":{"iopub.status.busy":"2023-06-06T03:45:48.672175Z","iopub.execute_input":"2023-06-06T03:45:48.672567Z","iopub.status.idle":"2023-06-06T03:45:49.128350Z","shell.execute_reply.started":"2023-06-06T03:45:48.672538Z","shell.execute_reply":"2023-06-06T03:45:49.127070Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**There are sentences that correspond to only a single frame, we will remove these sentences from our analysis as they are likely errors. It doesn't make sense for a lengthy phrase like a phone number to consist of a single sign language gesture. We will filter only those sentences that have more than 300 frames.**","metadata":{}},{"cell_type":"code","source":"for i in range(0,9):\n    index_count = dataframes[i].index.value_counts()\n\n    # Filtrar los índices que se repiten 300 o más veces\n    filtered_indices = index_count[index_count >= 300].index\n\n    # Filtrar el dataframe original\n    dataframes[i] = dataframes[i].groupby(dataframes[i].index).filter(lambda x: x.index[0] in filtered_indices)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-06T03:45:51.961578Z","iopub.execute_input":"2023-06-06T03:45:51.962079Z","iopub.status.idle":"2023-06-06T03:45:52.772604Z","shell.execute_reply.started":"2023-06-06T03:45:51.962043Z","shell.execute_reply":"2023-06-06T03:45:52.771439Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**We equalized the number of frames for each landmark sequence to 720 frames.**","metadata":{}},{"cell_type":"code","source":" for i in range(9):\n    sequence_id = dataframes[i].index.unique()\n    for j in range(len(sequence_id)):\n        current_frames = dataframes[i].index.value_counts()[sequence_id[j]]\n        num_new_rows = 720 - current_frames\n        if num_new_rows > 0:\n            new_rows = pd.DataFrame([[current_frames + 1 + k] + [0] * 84 for k in range(num_new_rows)],\n                                    columns=dataframes[0].columns, index=[sequence_id[j]] * num_new_rows)\n            dataframes[i] = pd.concat([dataframes[i], new_rows], axis=0, ignore_index=False)\n            \n    print(i)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:27:33.964912Z","iopub.execute_input":"2023-06-06T00:27:33.965536Z","iopub.status.idle":"2023-06-06T00:27:50.576773Z","shell.execute_reply.started":"2023-06-06T00:27:33.965490Z","shell.execute_reply":"2023-06-06T00:27:50.575683Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"0\n1\n2\n3\n4\n5\n6\n7\n8\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Remove null values**","metadata":{}},{"cell_type":"code","source":"for i in range(9):\n    dataframes[i] = dataframes[i].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:28:14.211536Z","iopub.execute_input":"2023-06-06T00:28:14.212770Z","iopub.status.idle":"2023-06-06T00:28:14.364136Z","shell.execute_reply.started":"2023-06-06T00:28:14.212720Z","shell.execute_reply":"2023-06-06T00:28:14.362505Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"dataframe = pd.concat(dataframes, ignore_index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:28:16.113847Z","iopub.execute_input":"2023-06-06T00:28:16.114346Z","iopub.status.idle":"2023-06-06T00:28:16.326388Z","shell.execute_reply.started":"2023-06-06T00:28:16.114301Z","shell.execute_reply":"2023-06-06T00:28:16.324924Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Relating each phrase to its respective dataframe","metadata":{}},{"cell_type":"markdown","source":"**Creating a dictionary with the identifiers of each phrase as key and as values the corresponding dataframe.**","metadata":{}},{"cell_type":"code","source":"dataframe_dict = {}\nfor sequence_id in dataframe.index.unique():\n    dataframe_dict[sequence_id] = dataframe.loc[sequence_id]","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:28:17.592600Z","iopub.execute_input":"2023-06-06T00:28:17.593052Z","iopub.status.idle":"2023-06-06T00:28:18.191515Z","shell.execute_reply.started":"2023-06-06T00:28:17.593020Z","shell.execute_reply":"2023-06-06T00:28:18.190428Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Create a list to store the landmark dataframes and another list to store the corresponding phrases in the same order.","metadata":{}},{"cell_type":"code","source":"indices = dataframe.index.unique()","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:28:25.584859Z","iopub.execute_input":"2023-06-06T00:28:25.585340Z","iopub.status.idle":"2023-06-06T00:28:25.596240Z","shell.execute_reply.started":"2023-06-06T00:28:25.585300Z","shell.execute_reply":"2023-06-06T00:28:25.594721Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"phrases = []\nfor index_phrase in indices:\n    phrase = train.query(f'sequence_id=={index_phrase}')['phrase'].values[0]\n    phrases.append(phrase)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:28:32.223696Z","iopub.execute_input":"2023-06-06T00:28:32.224848Z","iopub.status.idle":"2023-06-06T00:28:33.855817Z","shell.execute_reply.started":"2023-06-06T00:28:32.224802Z","shell.execute_reply":"2023-06-06T00:28:33.854796Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"sequence_phrase = {}\nfor i in range(len(phrases)):\n    sequence_id_phrase = indices[i] # Identificador de la secuencia\n    phrase = phrases[i]\n    sequence_phrase[sequence_id_phrase] = phrase","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:28:36.114450Z","iopub.execute_input":"2023-06-06T00:28:36.114878Z","iopub.status.idle":"2023-06-06T00:28:36.122054Z","shell.execute_reply.started":"2023-06-06T00:28:36.114844Z","shell.execute_reply":"2023-06-06T00:28:36.120764Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"landmarks_data = []  # Lista para almacenar los dataframes de landmarks\nphrase_order = []  # Lista para mantener el orden de las frases\n\n# Iterar sobre los sequence_id\nfor sequence_id, dataframe in dataframe_dict.items():\n    landmarks_dataframe = dataframe.iloc[:, 1:]  # Seleccionar todas las columnas excepto la última (frames)\n    landmarks_data.append(landmarks_dataframe)\n    phrase = sequence_phrase.get(sequence_id)  # Obtener la frase correspondiente al sequence_id\n    phrase_order.append(phrase)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:28:38.227095Z","iopub.execute_input":"2023-06-06T00:28:38.227563Z","iopub.status.idle":"2023-06-06T00:28:38.665174Z","shell.execute_reply.started":"2023-06-06T00:28:38.227529Z","shell.execute_reply":"2023-06-06T00:28:38.663833Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"**Now each landmark and phrase are related by the index in each of their respective lists.**","metadata":{}},{"cell_type":"markdown","source":"## Create a list to store the landmark dataframes and another list to store the corresponding letter in the same order.\n","metadata":{}},{"cell_type":"markdown","source":"### Code phrases","metadata":{}},{"cell_type":"code","source":"#Load the character_to_prediction_index.json file\nwith open('/kaggle/input/asl-fingerspelling/character_to_prediction_index.json') as f:\n    character_to_index = json.load(f)\n    \n# Create an index dictionary to characters\nindex_to_character = {v: k for k, v in character_to_index.items()}","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:30:18.289275Z","iopub.execute_input":"2023-06-06T00:30:18.289804Z","iopub.status.idle":"2023-06-06T00:30:18.299482Z","shell.execute_reply.started":"2023-06-06T00:30:18.289769Z","shell.execute_reply":"2023-06-06T00:30:18.297731Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"encoded_phrases = []\n\n# Iterar sobre las etiquetas\nfor phrase in phrase_order:\n    encoded_phrase = [character_to_index.get(c, 0) for c in phrase]\n    encoded_phrases.append(encoded_phrase)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:30:19.178239Z","iopub.execute_input":"2023-06-06T00:30:19.178766Z","iopub.status.idle":"2023-06-06T00:30:19.189224Z","shell.execute_reply.started":"2023-06-06T00:30:19.178716Z","shell.execute_reply":"2023-06-06T00:30:19.187710Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"**We converted the sentences and our input data into arrays. Since each sentence has a different length, we equalized their lengths by padding them to a length of 30 characters. We filled these characters with 0, as 0 represents an empty space and relates well to the data in the landmarks where there are zeros.**","metadata":{}},{"cell_type":"code","source":"array_phrases = np.array(encoded_phrases)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:30:20.675531Z","iopub.execute_input":"2023-06-06T00:30:20.675985Z","iopub.status.idle":"2023-06-06T00:30:20.682842Z","shell.execute_reply.started":"2023-06-06T00:30:20.675951Z","shell.execute_reply":"2023-06-06T00:30:20.681805Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_32/2265975440.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  array_phrases = np.array(encoded_phrases)\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.utils import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:32:58.387259Z","iopub.execute_input":"2023-06-06T00:32:58.387755Z","iopub.status.idle":"2023-06-06T00:32:58.395023Z","shell.execute_reply.started":"2023-06-06T00:32:58.387715Z","shell.execute_reply":"2023-06-06T00:32:58.393260Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Realiza el padding para igualar las longitudes de las frases\npadded_phrases = pad_sequences(array_phrases, maxlen=30, padding='post')","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:33:09.297027Z","iopub.execute_input":"2023-06-06T00:33:09.297434Z","iopub.status.idle":"2023-06-06T00:33:09.309141Z","shell.execute_reply.started":"2023-06-06T00:33:09.297403Z","shell.execute_reply":"2023-06-06T00:33:09.307537Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"padded_phrases=padded_phrases.astype(np.int16)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:33:18.116351Z","iopub.execute_input":"2023-06-06T00:33:18.116804Z","iopub.status.idle":"2023-06-06T00:33:18.122838Z","shell.execute_reply.started":"2023-06-06T00:33:18.116771Z","shell.execute_reply":"2023-06-06T00:33:18.121404Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"for i in range(len(landmarks_data)):\n    landmarks_data[i].reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:34:37.408472Z","iopub.execute_input":"2023-06-06T00:34:37.408920Z","iopub.status.idle":"2023-06-06T00:34:37.425094Z","shell.execute_reply.started":"2023-06-06T00:34:37.408887Z","shell.execute_reply":"2023-06-06T00:34:37.424074Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"landmarks_array = np.array(landmarks_data)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:35:07.239928Z","iopub.execute_input":"2023-06-06T00:35:07.240449Z","iopub.status.idle":"2023-06-06T00:35:07.730222Z","shell.execute_reply.started":"2023-06-06T00:35:07.240407Z","shell.execute_reply":"2023-06-06T00:35:07.728834Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"landmarks_array=landmarks_array.astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:35:10.906435Z","iopub.execute_input":"2023-06-06T00:35:10.906930Z","iopub.status.idle":"2023-06-06T00:35:10.983952Z","shell.execute_reply.started":"2023-06-06T00:35:10.906895Z","shell.execute_reply":"2023-06-06T00:35:10.982532Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"landmarks_array.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:35:24.756986Z","iopub.execute_input":"2023-06-06T00:35:24.757680Z","iopub.status.idle":"2023-06-06T00:35:24.768294Z","shell.execute_reply.started":"2023-06-06T00:35:24.757599Z","shell.execute_reply":"2023-06-06T00:35:24.766321Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"(575, 720, 84)"},"metadata":{}}]},{"cell_type":"code","source":"padded_phrases.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:37:12.321387Z","iopub.execute_input":"2023-06-06T00:37:12.321882Z","iopub.status.idle":"2023-06-06T00:37:12.331035Z","shell.execute_reply.started":"2023-06-06T00:37:12.321846Z","shell.execute_reply":"2023-06-06T00:37:12.329722Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"(575, 30)"},"metadata":{}}]},{"cell_type":"markdown","source":"### We store data to save resources","metadata":{}},{"cell_type":"code","source":"import h5py","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:40:51.211300Z","iopub.execute_input":"2023-06-06T00:40:51.211924Z","iopub.status.idle":"2023-06-06T00:40:51.218220Z","shell.execute_reply.started":"2023-06-06T00:40:51.211876Z","shell.execute_reply":"2023-06-06T00:40:51.216547Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"with h5py.File('/kaggle/working/landmarks_array.h5', 'w') as f:\n    f.create_dataset('landmarks_array', data=landmarks_array)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:40:52.234471Z","iopub.execute_input":"2023-06-06T00:40:52.235559Z","iopub.status.idle":"2023-06-06T00:40:52.562592Z","shell.execute_reply.started":"2023-06-06T00:40:52.235505Z","shell.execute_reply":"2023-06-06T00:40:52.561218Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"with h5py.File('/kaggle/working/y_data.h5', 'w') as f:\n    f.create_dataset('y_data', data=padded_phrases)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T00:41:21.484428Z","iopub.execute_input":"2023-06-06T00:41:21.484889Z","iopub.status.idle":"2023-06-06T00:41:21.493722Z","shell.execute_reply.started":"2023-06-06T00:41:21.484856Z","shell.execute_reply":"2023-06-06T00:41:21.492116Z"},"trusted":true},"execution_count":39,"outputs":[]}]}